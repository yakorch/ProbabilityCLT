### Task 2.
In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the __radioactive decay__ process.

Consider a sample of radioactive element of mass $m$, which has a big _half-life period_ \(T\); it is vitally important to know the probability that during a one-second period, the number of nuclei decays will not exceed some critical level \(k\). This probability can easily be estimated using the fact that, given the _activity_ ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass \(m = \mathtt{team\, id \,number} \times 10^{-6}\) g each. Denote by $X_1,X_2,\dots,X_n$ the __i.i.d.  r.v.__'s counting the number of decays in sample $i$ in one second.

### 1.  Specify the parameter of the Poisson distribution of \(X_i\) (you'll need the atomic mass of _Cesium-137_)


Atomic mass of Cesium-137 is $139 Da$

$1 Da = 1.660538921 × 10−24 gram$

Cesium-137: $136.907 g/mol$

$T = 30.1 years$

$λ = log(2)/T$ in seconds.

$N_A = 6 × 10^23$

$µ := Nλ$

$m = (teamidnumber) 9 × 10−6 g each$

$X1,X2,...,Xn$ -- the i.i.d. r.v.’s

$µ := Nλ$

$mole = m / m_A = 9 × 10−6 g / (136.907 g / mol)$

$N/N_A = mole; N = mole * N_A$

```{r}
N <- 9 * 10^(-6) * 6 * 10^(23) / (136.9)
lambda <- log(2) / (9.4996 * 10^8)
mu <- N * lambda
```


### 2.  Show that the distribution of the sample means of \(X_1,\dots,X_n\) gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    +  simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    +  repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function \(\hat  F_{\mathbf{s}}\) of $\mathbf{s}$;
    +  identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} \(F\) of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} \(\hat F_{\mathbf{s}}\) and plot both __c.d.f.__'s on one graph to visualize their proximity (use the proper scales!);
    +  calculate the maximal difference between the two \textbf{c.d.f.}'s;
    +  consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.


#### Plotting ecdf and cdf for different value of $n$:

```{r}

K <- 1e3
n <- 10

ens <- c(5, 10, 50)

for (ind in 1:3){
  n <- ens[ind]
  sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))

  mu <- mean(sample_means)
  variance <- var(sample_means)
  sigma <- sqrt(variance) # or sd(sample_means)

  xlims <- c(mu-3*sigma,mu+3*sigma)
  Fs <- ecdf(sample_means)
  plot(Fs,
     xlim = xlims,
     ylim = c(0,1),
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
  curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
}

```
As can be seen from the plots, the Poisson distribution's cdf is very similar to the normal distribution. And as $n$ becomes larger, the less the difference between them. Though the results are pretty good for small values of $n$.
This is a great example of CLT.

```{r}
ens <- c(5, 10, 50)
for (ind in 1:3){
    n <- ens[ind]
    sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
    x <- seq(mu - 3 * sigma, mu + 3 * sigma, by = 6 * sigma / 200)
    print(max(abs(ecdf(sample_means)(x) - (pnorm(x, mean = mu, sd = sigma)))))
}
```
As can be seen from these numbers: the bigger the sample, the less the deviation.
Which is logical since sample of Poisson distributions behaves like the Normal distribution only when $n->+\infty$. Which means less error as n increases.


3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,
    +  obtain the theoretical bound on \(n\) using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;
    +  simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    +  repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    +  calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level \(0.95\)

$a = 8 \times 10^8$

Let S r.v. be the total number of decays.

### Markov's inequality:

$P(S \le a) = 1 - P(S \ge a) \ge 1 - \frac{E(S)}{a}$

Since, samples are independent: $P(S \le a) \ge 1 - \frac{n \cdot E(S_0)}{a}$

$E(S_0)$ is the expectation of the r.v. with a poisson distribution, and is equal to lambda.

Then: $P(S \le a) \ge 1 - \frac{n \cdot \mu}{a} \approx 1 - \frac{n \cdot 28.8 \cdot 10^6}{8 \cdot 10^8} = 1 - \frac{3.6 \cdot n}{100}$

Markov's inequality gives that the largest $n$ will be 1. Well, it's not zero, thanks.



### Chernoff bound:

Similarly, we have: $P(S\le a) = 1 - P(S \ge a) = 1 - P(e^{tS} \ge e^{ta}) \ge 1 - \frac{E(e^{tS})}{e^{ta}} = 1 - \frac{n \cdot E(e^{tS_0})}{e^{ta}}$

Fact: $G_X(s) = e^{\mu(s-1)}$ for Poisson distribution.

$P(S\le a) \ge 1 - \frac{n \cdot e^{\mu (e^t - 1)}}{e^{ta}} = 1 - n\cdot \frac{e^{\mu \cdot (s - 1)}}{s^a}$.
Here let's take $s = 1$: $P(S\le a) \ge 1 - n$. Also did not give a good result.


### CLT:

$P(S \le a) =  P(\frac{S - \mu \cdot n}{\sqrt{n} \cdot \sigma}) \le \frac{a - \mu \cdot n}{\sqrt{n} \cdot \sigma} \approx \Phi(\frac{a - \mu \cdot n}{\sqrt{n} \cdot \sigma})$

Here the mean is $28.8 \times 10^6$, and variance is the same.

We have: $\Phi(\frac{a - \mu \cdot n}{\sqrt{n} \cdot \sigma}) \ge 0.95 \approx \Phi(1.65)$
$\Phi$ is non-decreasing => $\frac{a - \mu \cdot n}{\sqrt{n} \cdot \sigma} \ge 1.65$

Plugging values gives (approximate calculations): $\frac{27.78}{\sqrt{n}} - \sqrt{n} \ge 1.65$.
From this one can obtain: $n_{max} = 21$

And CLT is the closest one to the real result!





```{r}
max_n <- 50
emp_probability <- seq(1, max_n, by=1)

for (j in 1:max_n){
  n <- j
  a_lot_sums <- colSums(matrix(rpois(n*K, lambda = mu), nrow=n))
  emp_probability[j] <- length(a_lot_sums[a_lot_sums < (8 * 10^8)]) / K
}

```


We see that the real result is actually 27. Markov's inequality did not perform efficiently, while other have more accurate results.
```{r}
emp_probability
```
